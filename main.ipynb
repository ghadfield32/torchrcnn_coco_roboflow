{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "* must have roboflow dataset\n",
    "* must use coco_segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1) Load in CUDA\n",
    "2) load in Pytorch documents\n",
    "3) create coco custom image folder\n",
    "4) load in roboflow image set\n",
    "5) get classes out of it and the number of classes\n",
    "6) load in the model (R-CNN)\n",
    "7) Load in roboflow set into custom coco folder\n",
    "8) input them into dataloaders\n",
    "9) train model\n",
    "10) save model\n",
    "11) load model\n",
    "12) process a video from youtube utilizing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu121\n",
      "CUDA available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "#create going_modular repository\n",
    "create_directory(\"going_modular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required files from torchvision\n",
    "import requests\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\",\n",
    "    \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
    "]\n",
    "download_files(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "#!pip install roboflow\n",
    "\n",
    "\n",
    "#from roboflow import Roboflow\n",
    "#rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
    "#project = rf.workspace(\"ai-79z1a\").project(\"basketball_child\")\n",
    "#dataset = project.version(6).download(\"coco-segmentation\")\n",
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "import torch\n",
    "import requests\n",
    "import yt_dlp\n",
    "import utils\n",
    "import shutil\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def download_videos_from_youtube(video_urls, output_path):\n",
    "    \"\"\"\n",
    "    Downloads videos from YouTube.\n",
    "\n",
    "    Args:\n",
    "    video_urls (list): List of YouTube video URLs.\n",
    "    output_path (str): Directory where videos will be saved.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing lists of successful and failed downloads.\n",
    "    \"\"\"\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'best',\n",
    "        'outtmpl': output_path + '/%(title)s.%(ext)s',\n",
    "        'quiet': True\n",
    "    }\n",
    "\n",
    "    failed_downloads = []\n",
    "    successful_downloads = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        for url in video_urls:\n",
    "            try:\n",
    "                ydl.download([url])\n",
    "                print(f\"Successfully downloaded {url}\")\n",
    "                successful_downloads.append(url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "                failed_downloads.append(url)\n",
    "\n",
    "    return successful_downloads, failed_downloads\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_project(api_key, workspace, project_name, version):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(\"coco-segmentation\")\n",
    "    return dataset\n",
    "\n",
    "def download_files(urls):\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(url.split(\"/\")[-1], 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def construct_dataset_paths(project_name, version):\n",
    "    base_path = f\"{project_name}-{version}\"\n",
    "    train_annotation_path = f\"{base_path}/train/_annotations.coco.json\"\n",
    "    valid_annotation_path = f\"{base_path}/valid/_annotations.coco.json\"\n",
    "    test_annotation_path = f\"{base_path}/test/_annotations.coco.json\"\n",
    "\n",
    "    train_root_dir = f\"{base_path}/train\"\n",
    "    valid_root_dir = f\"{base_path}/valid\"\n",
    "    test_root_dir = f\"{base_path}/test\"\n",
    "\n",
    "    return train_annotation_path, valid_annotation_path, test_annotation_path, train_root_dir, valid_root_dir, test_root_dir\n",
    "\n",
    "def create_directory(dir_path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(dataset, split_ratio=0.8):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * split_ratio)\n",
    "    valid_size = total_size - train_size\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(42))\n",
    "    return train_dataset, valid_dataset\n",
    "        \n",
    "def delete_folder_and_video(folder_path, video_path):\n",
    "    \"\"\"\n",
    "    Delete the specified folder and video.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path: Path object or str, the path to the folder to delete.\n",
    "    - video_path: Path object or str, the path to the video file to delete.\n",
    "    \"\"\"\n",
    "    if folder_path.exists():\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    if video_path.exists():\n",
    "        os.remove(video_path)\n",
    "        print(f\"Deleted video: {video_path}\")\n",
    "    else:\n",
    "        print(f\"Video not found: {video_path}\")\n",
    "        \n",
    "def load_classes_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are class IDs and values are class names.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    categories = data['categories']\n",
    "    classes = {category['id']: category['name'] for category in categories}\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/coco_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/coco_dataset.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_path, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(annotation_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # Filter out images without annotations\n",
    "        annotated_images = []\n",
    "        for img in self.annotations['images']:\n",
    "            image_id = img['id']\n",
    "            anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "            if len(anns) > 0:\n",
    "                annotated_images.append(img)\n",
    "\n",
    "        self.image_ids = [img['id'] for img in annotated_images]\n",
    "\n",
    "        # Update the self.annotations['images'] to include only annotated images\n",
    "        self.annotations['images'] = annotated_images\n",
    "        \n",
    "        #print(\"Number of images:\", len(self.annotations['images']))\n",
    "        #print(\"Sample image entry:\", self.annotations['images'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.annotations['images'][idx]\n",
    "        image_id = img_info['id']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "        #print(\"Image size (PIL):\", img.size)\n",
    "        #print(\"Image shape (tensor):\", img_tensor.shape)\n",
    "\n",
    "        anns = [ann for ann in self.annotations['annotations'] if ann['image_id'] == image_id]\n",
    "        #print(\"Number of annotations for this image:\", len(anns))\n",
    "\n",
    "        boxes = [ann['bbox'] for ann in anns]  # bbox format: [x_min, y_min, width, height]\n",
    "        # Convert from XYWH to XYXY format\n",
    "        boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in boxes]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = [ann['category_id'] for ann in anns]\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        #print(\"Boxes shape:\", boxes.shape)\n",
    "        #print(\"Labels:\", labels)\n",
    "        # Debug print\n",
    "        #print(f\"Boxes shape for image {idx}: {boxes.shape}\")\n",
    "\n",
    "        masks = []\n",
    "        for ann in anns:\n",
    "            if 'segmentation' in ann and isinstance(ann['segmentation'], list):\n",
    "                for seg in ann['segmentation']:\n",
    "                    mask_img = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
    "                    ImageDraw.Draw(mask_img).polygon(seg, outline=1, fill=1)\n",
    "                    mask = np.array(mask_img)\n",
    "                    masks.append(mask)\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8) if masks else torch.zeros((0, img_info['height'], img_info['width']), dtype=torch.uint8)\n",
    "        #print(\"Masks shape:\", masks.shape)\n",
    "\n",
    "        areas = [ann['area'] for ann in anns]\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = [ann['iscrowd'] for ann in anns]\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Convert masks to Mask format\n",
    "        masks = tv_tensors.Mask(masks)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id  # Changed to integer\n",
    "        target[\"area\"] = areas\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        #print(\"Target:\", target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_tensor, target = self.transforms(img_tensor, target)\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/visualization_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/visualization_utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F  # Add this import\n",
    "\n",
    "# New function to visualize transformations\n",
    "def visualize_transformation(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    transformed_img, transformed_target = dataset.transforms(img, target)\n",
    "    original_img = F.to_pil_image(img)\n",
    "    transformed_img = F.to_pil_image(transformed_img)\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    # Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_img)\n",
    "    for box in target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Original Image - ID: {idx}\")\n",
    "\n",
    "    # Transformed Image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(transformed_img)\n",
    "    for box in transformed_target[\"boxes\"]:\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='b', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        #print(x_min, y_min, x_max, y_max)\n",
    "    plt.title(f\"Transformed Image - ID: {idx}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_bbox(dataset, idx):\n",
    "    img, target = dataset[idx]\n",
    "    original_img = F.to_pil_image(img)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(original_img)\n",
    "\n",
    "    for box in target[\"boxes\"]:  # Access the boxes directly\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        # Debug print\n",
    "        print(f\"Visualizing BBox - xmin: {x_min}, ymin: {y_min}, xmax: {x_max}, ymax: {y_max}\")\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.title(f\"Image with Bounding Boxes - ID: {idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/model_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_utils.py\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import utils\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def load_classes_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads the class names and their corresponding IDs from a COCO format JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are class IDs and values are class names.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    categories = data['categories']\n",
    "    classes = {category['id']: category['name'] for category in categories}\n",
    "    return classes\n",
    "\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "#classes = load_classes_from_json('basketball_child-6/test/_annotations.coco.json')\n",
    "#print(classes)\n",
    "\n",
    "# model_utils.py\n",
    "def get_model_instance_segmentation(num_classes, hidden_layer=256):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_to_huggingface(model_directory, model_id):\n",
    "    \"\"\"Upload the model to Hugging Face Hub.\"\"\"\n",
    "    hf_api = HfApi()\n",
    "    username = hf_api.whoami()['name']\n",
    "    repo_name = f\"{username}/{model_id}\"\n",
    "    repo_url = hf_api.create_repo(repo_name, exist_ok=True, private=False)\n",
    "\n",
    "    repo = Repository(local_dir=model_directory, clone_from=repo_url, use_auth_token=True)\n",
    "    repo.lfs_track([\"*.bin\", \"*.pth\", \"*.ckpt\"])  # Track large model files with Git LFS\n",
    "    repo.git_add()\n",
    "    repo.git_commit(\"Initial commit of the model\")\n",
    "    try:\n",
    "        repo.git_push()\n",
    "        print(f\"Model successfully uploaded to: {repo_url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload model to Hugging Face: {e}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/transforms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/transforms.py\n",
    "import torch  # Add this import statement\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, ConvertImageDtype\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    #if train:\n",
    "    #    transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def transform_image(image_bytes):\n",
    "    \"\"\"\n",
    "    Transforms image bytes into a tensor with the correct format for the model.\n",
    "    \n",
    "    Args:\n",
    "    image_bytes (bytes): The image in bytes format, as uploaded by the user.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The transformed image as a tensor.\n",
    "    \"\"\"\n",
    "    # Define the transformations\n",
    "    my_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to the size required by your model\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard normalization for pre-trained models\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load the image from bytes and apply transformations\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    return my_transforms(image).unsqueeze(0)  # Add a batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "# train.py\n",
    "import torch\n",
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "\n",
    "def train_model(model, data_loader, data_loader_valid, device, num_epochs,\n",
    "                lr=0.005, momentum=0.9, weight_decay=0.0005, step_size=3, gamma=0.1):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_valid, device=device)\n",
    "\n",
    "    #torch.save(model.state_dict(), 'results/models/model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/process_video_check.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/process_video_check.py\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "def intersects(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1.tolist()\n",
    "    x2_min, y2_min, x2_max, y2_max = box2.tolist()\n",
    "    return (x1_min < x2_max and x1_max > x2_min and y1_min < y2_max and y1_max > y2_min)\n",
    "\n",
    "def process_video_check(video_path, model, device, classes, classes_to_track=None, threshold=0.5, check_intersections=False):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    score = 0  # Initialize the score counter\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_tensor = T.ToTensor()(frame).unsqueeze_(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(frame_tensor)[0]\n",
    "\n",
    "        pred_scores = prediction['scores']\n",
    "        pred_boxes = prediction['boxes']\n",
    "        pred_labels = prediction['labels']\n",
    "        pred_masks = prediction['masks']\n",
    "\n",
    "        keep = pred_scores > threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "        pred_masks = pred_masks[keep]\n",
    "\n",
    "        #print(f\"Original Frame Size: {frame.shape}\")\n",
    "        #print(f\"Pred Boxes before drawing: {pred_boxes}\")\n",
    "\n",
    "        if not keep.any():\n",
    "            continue  # Skip this frame if no detections are kept\n",
    "\n",
    "        # Convert numeric labels to class names\n",
    "        pred_class_names = [classes[label.item()] for label in pred_labels]\n",
    "\n",
    "        if check_intersections == True and classes_to_track:\n",
    "            # Perform intersection checks only if enabled and classes_to_track is specified\n",
    "            for class_pair in classes_to_track:\n",
    "                class1_boxes = pred_boxes[[name == class_pair[0] for name in pred_class_names]]\n",
    "                class2_boxes = pred_boxes[[name == class_pair[1] for name in pred_class_names]]\n",
    "\n",
    "                for box1 in class1_boxes:\n",
    "                    for box2 in class2_boxes:\n",
    "                        if intersects(box1, box2):\n",
    "                            score += 1\n",
    "                            print(f\"Intersection detected between {class_pair[0]} and {class_pair[1]}, Score:\", score)\n",
    "\n",
    "        # Frame Tensor Conversion for Drawing\n",
    "        frame_tensor = (255.0 * (frame_tensor - frame_tensor.min()) / (frame_tensor.max() - frame_tensor.min())).to(torch.uint8)\n",
    "        frame_tensor = frame_tensor.squeeze().to(torch.uint8)\n",
    "\n",
    "        # Draw bounding boxes and segmentation masks\n",
    "        output_image = draw_bounding_boxes(frame_tensor, pred_boxes, labels=pred_class_names, colors=\"red\")\n",
    "        output_image = draw_segmentation_masks(output_image, (pred_masks > 0.7).squeeze(1), alpha=0.5, colors=\"blue\")\n",
    "\n",
    "        # Convert output image for displaying\n",
    "        output_image = output_image.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "        output_image = np.clip(output_image, 0, 255)  # Ensure values are within 0-255\n",
    "        if check_intersections == True and classes_to_track:\n",
    "            # Draw score text\n",
    "            cv2.putText(output_image, f'Score: {score}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Frame', output_image)\n",
    "        \n",
    "        # Just before cv2.imshow\n",
    "        #print(f\"Output Image Size: {output_image.shape}\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "# process_video_check(video_path, model, device, classes, [('ball', 'rim')], threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from going_modular.utils import (get_device, create_directory \n",
    "                                ,get_project, download_videos_from_youtube\n",
    "                                , delete_folder_and_video, load_classes_from_json\n",
    "                                ,split_dataset)\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import (get_model_instance_segmentation\n",
    "                                        , upload_to_huggingface)\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.transforms import get_transform\n",
    "from going_modular.process_video_check import process_video_check\n",
    "import utils\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    data_path = Path(args.data_path)\n",
    "    model_path = Path(args.model_path)\n",
    "    create_directory(data_path)\n",
    "    create_directory(model_path)\n",
    "\n",
    "    # Check if the project data is already downloaded\n",
    "    project_folder = Path(f'{args.project_folder_name}-{args.version}')\n",
    "    classes_path = project_folder / 'train' / '_annotations.coco.json'\n",
    "\n",
    "    if not project_folder.exists() or not classes_path.exists():\n",
    "        print(\"Downloading project data...\")\n",
    "        get_project(args.api_key, args.workspace, args.project_name, args.version)\n",
    "\n",
    "    # Load classes from JSON\n",
    "    classes = load_classes_from_json(classes_path)\n",
    "    print(\"Classes loaded:\", classes)\n",
    "\n",
    "    num_classes = len(classes) + 1\n",
    "    device = get_device()\n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=args.hidden_layer)\n",
    "    model.to(device)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        video_filename = args.video_name if args.video_name.endswith('.mp4') else f\"{args.video_name}.mp4\"\n",
    "        video_path = Path(args.data_path) / video_filename\n",
    "        # Load datasets\n",
    "        datasets = {}\n",
    "        data_loaders = {}\n",
    "        for dtype in ['train', 'valid', 'test']:\n",
    "            ann_path = project_folder / dtype / '_annotations.coco.json'\n",
    "            img_dir = project_folder / dtype\n",
    "            if ann_path.exists() and img_dir.exists():\n",
    "                datasets[dtype] = CustomCocoDataset(str(ann_path), str(img_dir), transforms=get_transform(train=dtype=='train'))\n",
    "                batch_size = 2 if dtype == 'train' else 1\n",
    "                data_loaders[dtype] = DataLoader(datasets[dtype], batch_size=batch_size, shuffle=dtype=='train', num_workers=0, collate_fn=utils.collate_fn)\n",
    "                print(f\"{dtype.capitalize()} dataset loaded.\")\n",
    "            else:\n",
    "                print(f\"{dtype.capitalize()} dataset not found or incomplete. Skipping.\")\n",
    "\n",
    "        if 'train' in datasets and 'valid' not in datasets:\n",
    "            print(\"Splitting dataset into train and valid...\")\n",
    "            train_dataset, valid_dataset = split_dataset(datasets['train'])\n",
    "            data_loaders['train'] = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "            data_loaders['valid'] = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "\n",
    "        if 'train' in data_loaders and 'valid' in data_loaders:\n",
    "            print(\"Starting training process...\")\n",
    "            train_model(model, data_loaders['train'], data_loaders.get('valid'), device, args.num_epochs, lr=args.lr)\n",
    "            model_file_path = model_path / 'model_weights.pth'\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "            print(f\"Model saved at: {model_file_path}\")\n",
    "        else:\n",
    "            print(\"Training failed. No valid data loaders available.\")\n",
    "            \n",
    "        if args.delete_folder_and_video == True:\n",
    "            delete_folder_and_video(project_folder, video_path)\n",
    "\n",
    "    # Process video if mode is 'process_video'\n",
    "    elif args.mode == 'process_video':\n",
    "        video_filename = args.video_name if args.video_name.endswith('.mp4') else f\"{args.video_name}.mp4\"\n",
    "        video_path = Path(args.data_path) / video_filename\n",
    "        if not video_path.exists():\n",
    "            print(\"Downloading video...\")\n",
    "            download_videos_from_youtube([args.video_url], str(Path(args.data_path)))\n",
    "\n",
    "        if video_path.exists():\n",
    "            model_file_path = model_path / 'model_weights.pth'\n",
    "            if model_file_path.exists():\n",
    "                model.load_state_dict(torch.load(str(model_file_path), map_location=device))\n",
    "                classes_to_track = list(zip(args.classes_to_track[::2], args.classes_to_track[1::2]))  # Convert flat list to list of tuples\n",
    "                process_video_check(video_path, model, device, classes, classes_to_track, args.threshold, args.check_intersections)\n",
    "            else:\n",
    "                print(\"Model weights file not found. Please train the model first.\")\n",
    "        if args.delete_folder_and_video == True:\n",
    "            delete_folder_and_video(project_folder, video_path)\n",
    "            \n",
    "    elif args.mode == 'hf_upload':\n",
    "        hf_login()  # Ensure user is logged in\n",
    "\n",
    "        # Automatically determine model directory (or you can still ask the user)\n",
    "        model_directory = args.model_path  # Assuming this is where your model is saved\n",
    "        if not Path(model_directory).exists():\n",
    "            print(f\"Model directory {model_directory} does not exist. Please specify a valid model directory.\")\n",
    "            return\n",
    "\n",
    "        model_id = input(\"Enter a name for your model on Hugging Face (e.g., my-cool-model): \")\n",
    "        try:\n",
    "            upload_to_huggingface(model_directory, model_id)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during model upload: {e}\")\n",
    "            \n",
    "        if args.delete_folder_and_video == True:\n",
    "            delete_folder_and_video(project_folder, video_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train a model for object detection or process video\")\n",
    "    parser.add_argument('--api_key', type=str, default=\"your_roboflow_api_key\", help='API key for Roboflow')\n",
    "    parser.add_argument('--workspace', type=str, default=\"your_roboflow_workspace\", help='Workspace name in Roboflow')\n",
    "    parser.add_argument('--project_name', type=str, default=\"your_roboflow_project\", help='Project name in Roboflow')\n",
    "    parser.add_argument('--project_folder_name', type=str, default=\"your_roboflow_project_folder_name\", help='Project folder name in Roboflow')\n",
    "    parser.add_argument('--version', type=int, default=1, help='Version of the dataset in Roboflow')\n",
    "    parser.add_argument('--hidden_layer', type=int, default=256, help='Hidden layer size for the MaskRCNN predictor')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Learning rate')\n",
    "    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs to train the model')\n",
    "    parser.add_argument('--video_url', type=str, default=\"https://www.youtube.com/watch?v=example_video_id\", help='URL of the video to process')\n",
    "    parser.add_argument('--video_name', type=str, default=\"your_youtube_video_name\", help='Name of the video file (with extension) to process')\n",
    "    parser.add_argument('--threshold', type=float, default=0.6, help='Detection threshold for process_video')\n",
    "    parser.add_argument('--display_video', type=bool, default=True, help='Whether to display the video during processing')\n",
    "    parser.add_argument('--data_path', type=str, default='results/data', help='Path to save downloaded data')\n",
    "    parser.add_argument('--model_path', type=str, default='results/models', help='Path to save model weights')\n",
    "    parser.add_argument('--mode', type=str, choices=['train', 'process_video', 'hf_upload'], default='train', help='Mode of operation: train or process_video')\n",
    "    parser.add_argument('--delete_folder_and_video', type=bool, default=False, help='Whether to delete the image folder and download video after use')\n",
    "    parser.add_argument('--check_intersections', type=bool, default=False , help='Enable intersection checks in video processing')\n",
    "    parser.add_argument('--classes_to_track', nargs='+', help='Classes to track for intersections, specified as pairs', default=[])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage\n",
    "!python train.py --api_key <api_key> \\\n",
    "                --workspace basketball-formations \\\n",
    "                --project_name basketball-and-hoop-7xk0h \\\n",
    "                --project_folder_name basketball-and-hoop \\\n",
    "                --version 11 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 1 \\\n",
    "                --threshold 0.6 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=kh7s2tGvswc&t=1s\" \\\n",
    "                --video_name \"Devin Booker Sets Record, Wins Three-Point Contest\" \\\n",
    "                --delete_folder_and_video True \\\n",
    "                --mode train \n",
    "\n",
    "                #--check_intersections True \\\n",
    "                #--classes_to_track Basketball Hoop \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "#Car Model Ex:\n",
    "#classes\n",
    "\n",
    "!python train.py --api_key <api_key> \\\n",
    "                --workspace netventure \\\n",
    "                --project_name car_segment-yarm8 \\\n",
    "                --project_folder_name car_segment \\\n",
    "                --version 1 \\\n",
    "                --hidden_layer 256 \\\n",
    "                --lr 0.005 \\\n",
    "                --num_epochs 10 \\\n",
    "                --threshold 0.8 \\\n",
    "                --video_url \"https://www.youtube.com/watch?v=boVidZ2K-QI\" \\\n",
    "                --video_name \"Driving for 1 minute\" \\\n",
    "                --delete_folder_and_video True \\\n",
    "                --mode train \n",
    "\n",
    "\n",
    "                #--check_intersections True \\\n",
    "                #--classes_to_track Car Plane \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "import torch\n",
    "import utils\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import modular functions (make sure all functions are correctly imported)\n",
    "from going_modular.utils import (get_device, create_directory, get_project,\n",
    "                                 download_files, construct_dataset_paths,\n",
    "                                 download_videos_from_youtube)\n",
    "from going_modular.coco_dataset import CustomCocoDataset\n",
    "from going_modular.model_utils import get_model_instance_segmentation, load_classes_from_json\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.process_video_check import process_video_check\n",
    "from going_modular.transforms import get_transform, transform_image\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Global variables (Consider storing and accessing these more securely and flexibly)\n",
    "MODEL_PATH = Path('results/models/model_weights.pth')\n",
    " \n",
    "\n",
    "# Ensure MODEL_PATH directory exists\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_model(num_classes: int, model_path: Path = MODEL_PATH):\n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=256)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Welcome to the API!\"}\n",
    "\n",
    "@app.post(\"/train\")\n",
    "async def train(api_key: str, workspace: str, project_name: str, project_folder_name: str, version: int, num_epochs: int = 10):\n",
    "    # Set up device\n",
    "    device = get_device()\n",
    "    \n",
    "    # Ensure data and model directories exist\n",
    "    data_path = Path('results/data')\n",
    "    model_path = Path('results/models')\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download and prepare the dataset\n",
    "    dataset = get_project(api_key, workspace, project_name, version)\n",
    "    train_annotation_path, valid_annotation_path, test_annotation_path, train_image_dir, valid_image_dir, test_image_dir = construct_dataset_paths(project_folder_name, version)\n",
    "    \n",
    "    # Load class names and set up the model\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    \n",
    "    model = get_model_instance_segmentation(num_classes, hidden_layer=256)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = CustomCocoDataset(train_annotation_path, train_image_dir, transforms=get_transform(train=True))\n",
    "    valid_dataset = CustomCocoDataset(valid_annotation_path, valid_image_dir, transforms=get_transform(train=False))\n",
    "    \n",
    "    # Set up data loaders\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=utils.collate_fn)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_data_loader, valid_data_loader, device, num_epochs)\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    return {\"message\": \"Model trained and saved successfully\"}\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(project_folder_name: str, version: int, file: UploadFile = File(...)):\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    model = load_model(num_classes)\n",
    "    \n",
    "    image_bytes = await file.read()\n",
    "    tensor = transform_image(image_bytes)\n",
    "    \n",
    "    # Prediction logic\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        top_prob, top_catid = probabilities.topk(1, dim=1)\n",
    "    \n",
    "    predicted_class = classes[top_catid.item()]\n",
    "    confidence = top_prob.item()\n",
    "    \n",
    "    return JSONResponse(content={\"class\": predicted_class, \"confidence\": confidence})\n",
    "\n",
    "from fastapi import UploadFile, File\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "@app.post(\"/process_video\")\n",
    "async def process_video(project_folder_name: str, version: int, video_file: UploadFile = File(...)):\n",
    "    # Define where to save the video temporarily\n",
    "    temp_video_path = Path(\"temp_videos\") / video_file.filename\n",
    "    temp_video_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save the uploaded video to the temporary path\n",
    "    with temp_video_path.open(\"wb\") as buffer:\n",
    "        shutil.copyfileobj(video_file.file, buffer)\n",
    "    \n",
    "    # Load model and classes for processing\n",
    "    CLASSES_JSON = Path(f'{project_folder_name}-{version}/test/_annotations.coco.json')\n",
    "    classes = load_classes_from_json(CLASSES_JSON)\n",
    "    num_classes = len(classes) + 1\n",
    "    model = load_model(num_classes)\n",
    "    \n",
    "    # Assuming process_video_check is adapted to return a meaningful result\n",
    "    # For example, modifying process_video_check to accept a video path and return a dictionary of results\n",
    "    results = process_video_check(str(temp_video_path), model, get_device(), classes, [('Basketball', 'Hoop')], threshold=0.6)\n",
    "\n",
    "    # Optionally, delete the temporary video file after processing\n",
    "    temp_video_path.unlink(missing_ok=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "\n",
    "#!uivcorn app:app --reload\n",
    "\n",
    "#For looking at all the options**\n",
    "#http://127.0.0.1:8000/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
